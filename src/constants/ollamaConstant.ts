export const OLLAMA_LOCALHOST = "http://localhost:11434";
export const OLLAMA_URL_CHAT = "http://localhost:11434/api/chat";
export const OLLAMA_RESPONSE_DATA = "Ollama is running";
export const OLLAMA_ON = {
  DATA: "data",
  END: "end",
  ERROR: "error",
};

export const OLLAMA_MSG_INFO = {
  MODEL_FOUND: "Models found: ",
  MODEL_NOT_FOUND:
    "Please ensure that you pull the models from the Ollama server.\nYou can do this by running the command 'ollama pull <MODEL_NAME>' in your terminal.\n check documentation for more information.",
  MODEL_SET_TO: "Model set to: ",
};

export const OLLAMA_MSG_ERROR = {
  OLLAMA_NOT_RUNNING: "Failed: Ollama is not running",
  OLLAMA_NOT_OR_NOT_INSTALLED:
    "Failed to check if Ollama is running. Please ensure Ollama is installed and check if Ollama serve is running, try again.",
};

export const OLLAMA_SETTING = {
  TITLES: {
    SETTINGS: "Settings",
    MODEL_LIST: "List",
  },
  MENU: {
    MODEL: "MODELS",
    PARAMETERS: "PARAMETERS",
  },
  SUB_MENU: {
    NUMBER_PREDICTION: "Number Prediction",
    WIN_SIZE: "Window Size",
    KEY_COMPLETION: "Key Completion",
    PREVIEW: "Preview response",
    MAX_TOKENS: "Max Tokens predicted",
    DELAY: "Delay",
    INLINE: "Inline",
    TEMPERATURE: "Temperature",
  },
};

export const OLLAMA_DES = {
  NUMBER_PREDICTION: "The maximum number of tokens generated by the model. default(1000)",
  WIN_SIZE:
    "The size of the prompt in characters. NOT tokens, so can be set about 1.5-2x the max tokens of the model (varies).",
  KEY_COMPLETION: "You can use the key completion space, (,) or another sign default is (SPACE).",
  PREVIEW: "whether to preview the response or not.",
  MAX_TOKENS:
    "The maximum number of tokens generated by the model for the response preview. Typically not reached as the preview stops on newline. Recommended to keep very low due to computational cost.",
  DELAY:
    "Time to wait in seconds before starting inline preview generation.\nPrevents Ollama server from running briefly every time the completion key is pressed, which causes unnecessary compute usage.\nIf you are not on a battery powered device, set this to 0 for a more responsive experience.",
  INLINE:
    "Ollama continues autocompletion after what is previewed inline.\nDisabling disables that feature as some may find it irritating.\nMultiline completion is still accessible through the shortcut even after disabling.",
  TEMPERATURE:
    "Temperature of the model. It is recommended to set it lower than you would for dialogue.",
};

export const OLLAMA_COMMAND = {
  TITLE: "OllamaScriptCode Autocomplete",
  PROGRESS: "Starting model...",
  CANCEL: "Autocompletion request terminated by user cancel",
  GENERATING: "Generating...",
  FINISHED: "Autocompletion request completed",
  ERROR: "Failed to generate autocompletion",
  COMPLETE: "Autocompletion with OllamaScriptCode",
  PRESS: "Press `Enter` to get an autocompletion from OllamaScriptCode",
};

export const OLLAMA_ROLES = {
  USER: "user",
  ASSISTANT: "assistant",
  SYSTEM: "system",
};

export const MODEL_LIST = {
  LlAVA: "llava",
};
